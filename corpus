#!/usr/bin/env ruby
require 'feature_scorer'
require 'cue_wrapper' if RUBY_PLATFORM == 'java'
require 'thor'
require 'zip/zip'
require 'thread/pool'
require 'yaml'
require 'base64'

class AnalysisHelper
  def initialize(analysis)
    @analysis = analysis
  end

  def literals
    @analysis.entry.token_interpretations.map { |ti| ti.text.downcase }
  end

  def concepts
    @analysis.entry.token_interpretations.flat_map { |ti| ti.concepts.map { |c| c.to_s } }
  end

  def ocfs
    @analysis.entry.token_interpretations.flat_map { |ti| ti.orthographically_correct.map { |ocf| ocf.text } }
  end

  def features
    concepts.map { |c| {text: c, type: :concept} } +
        ocfs.map { |ocf| {text: ocf, type: :ocf} } +
        literals.map { |l| {text: l, type: :literal} }
  end
end

class ScoreExporter

  def initialize(evaluator, output_file)
    @evaluator = evaluator
    @output_file = output_file
  end

  def open
    @file = File.open(@output_file, 'w')
  end

  def close
    @file.close if @file
  end

  def write_header(tags)
    @file.puts "Feature|Type|Score|Tag|N11|N10|N01|N00|Uniq x Doc|Freq Abs #{tags[0]}|Freq Abs #{tags[1]}"
  end

  def write(feature, corpus)
    m = corpus.occurrence_matrices[feature]
    @file.puts "\"#{feature.text}\"|#{feature.type}|#{@evaluator.score(feature, corpus)}|#{@evaluator.relevant_tag(feature, corpus)}|#{m.n11}|#{m.n10}|#{m.n01}|#{m.n00}|#{m.n1_}|#{m.specific_occurrences}|#{m.residue_occurrences}"
  end

end

class Corpus < Thor

  desc 'create', 'Create corpus from CSV file [text, tag]'
  method_option :rows, type: :numeric, aliases: '-r', desc: 'Rows limit'
  method_option :threads, type: :numeric, aliases: '-t', desc: 'Thread pool size'

  def create(csv_file)
    puts "Creating corpus from #{csv_file}"
    tmp_path = "/tmp/#{Time.now.to_i}"
    basename = File.basename(csv_file, '.*')
    path = "#{tmp_path}/#{basename}"

    FileUtils.mkpath path

    csv = File.open(csv_file, 'r') { |f| f.readlines }
    rows_limit = options[:rows] || (csv.count - 1)
    rows_processed = 0

    cue = CueWrapper::Cue.new language: :english
    runner = cue.send :cue

    pool = Thread::Pool.new(options[:threads])
    mutex = Mutex.new

    start = Time.now
    1.upto(rows_limit) do |i|
      pool.process do
        text, tag = csv[i].split('|')
        puts "Analyzing #{i}/#{rows_limit}: #{text[0..60]}"
        analysis = AnalysisHelper.new(runner.run(cue.send(:request, text)))
        features = analysis.features.map { |f| FeatureScorer::Feature.new(f[:text], f[:type]) }
        File.open("#{path}/#{i}.bin", 'w') { |f| f.write serialize(FeatureScorer::Document.new(features, tag.strip)) }
        mutex.synchronize { rows_processed += 1 }
        puts "Complete #{rows_processed}/#{rows_limit}: #{text[0..60]}"
      end
    end

    loop do
      break if rows_processed == rows_limit
    end
    finish = Time.now

    pool.shutdown

    zip path, "#{basename}_#{rows_limit}rows.corpus"

    FileUtils.rm_rf tmp_path
    puts "Time: #{finish - start}"
  end

  desc 'scores', 'Calculate scores for corpus file'

  def scores(corpus_file)
    start = Time.now
    corpus = load_corpus(corpus_file)

    puts "Detected tags: #{corpus.tags.join(', ')}"
    if corpus.tags.count < 2
      puts "Can't calculate feature selection if corpus has less than two tags"
      return
    end

    puts 'Calculating score'
    exporters = [
        ScoreExporter.new(FeatureScorer::MutualInformationEvaluator.new, "#{File.basename(corpus_file, '.*')}_mi_scores.csv"),
        ScoreExporter.new(FeatureScorer::ChiSquareEvaluator.new, "#{File.basename(corpus_file, '.*')}_chi_scores.csv"),
        ScoreExporter.new(FeatureScorer::TfIdfEvaluator.new, "#{File.basename(corpus_file, '.*')}_tf_idf_scores.csv"),
        ScoreExporter.new(FeatureScorer::PlainFrequencyEvaluator.new, "#{File.basename(corpus_file, '.*')}_plain_scores.csv")
    ]

    exporters.each do |e|
      e.open
      e.write_header corpus.tags
    end

    uniq_features = corpus.features.uniq
    i = 0
    uniq_features.each do |feature|
      puts "Processing feature (#{i+=1}/#{uniq_features.count}): #{feature}"
      exporters.each { |e| e.write feature, corpus }
    end

    exporters.each { |e| e.close }

    File.open("#{File.basename(corpus_file, '.*')}_info.txt", 'w') do |f|
      specific_tag_count = corpus.select { |d| d.tag == corpus.tags[0] }.count
      residue_tag_count = corpus.select { |d| d.tag == corpus.tags[1] }.count
      uniq_features = corpus.features.uniq
      f.puts 'Tags:'
      f.puts "- #{corpus.tags[0]}: (specific) #{specific_tag_count}"
      f.puts "- #{corpus.tags[1]}: (specific) #{residue_tag_count}"

      f.puts "Documents: #{corpus.count}"

      f.puts 'Features (uniq):'
      f.puts "- Literal: #{uniq_features.select { |f| f.type == :literal }.count}"
      f.puts "- Concept: #{uniq_features.select { |f| f.type == :concept }.count}"
      f.puts "- OCF: #{uniq_features.select { |f| f.type == :ocf }.count}"

      tokens_count = corpus.features.select { |f| f.type == :literal }.count
      f.puts "Tokens: #{tokens_count}"
      f.puts "Token Avg: #{tokens_count/corpus.count}"

      f.puts 'Histogram:'
      histogram(corpus).each do |k, v|
        f.puts "- #{k} tokens: #{v} documents"
      end
    end

    puts "Time: #{Time.now - start}"
  end

  private

  def zip(folder, zip_file)
    puts 'Compressing corpus'
    File.delete zip_file if File.exists?(zip_file)
    Zip::ZipFile.open(zip_file, Zip::ZipFile::CREATE) do |zip|
      Dir.glob("#{folder}/*.*").each do |f|
        puts " - #{f}"
        zip.add(File.basename(f), f)
      end
    end
  end

  def load_corpus(corpus_file)
    corpus = nil
    Zip::ZipFile.open(corpus_file, Zip::ZipFile::CREATE) do |zip|
      puts 'Loading corpus'
      i = 0
      count = zip.entries.count
      documents = zip.entries.map do |f|
        puts "Parsing (#{i+=1}/#{count}): #{f.name}"
        File.extname(f.name) == '.bin' ? deserialize(zip.read(f.name)) : YAML.load(zip.read(f.name))
      end
      corpus = FeatureScorer::Corpus.new(documents)
    end
    corpus
  end

  def histogram(corpus)
    documents = corpus.map { |d| d.features.select { |f| f.type == :literal }.count }.sort

    slot = ((documents.last - documents.first).to_f / 10).round(1)

    previous = documents.first

    1.upto(10).inject({}) do |hash, i|
      current = (previous + slot).round(1)
      hash["#{previous.to_i} to #{current.to_i}"] = documents.select { |n| n >= previous && n < current }.count
      hash["#{previous.to_i} to #{current.to_i}"] += 1 if i == 10
      previous = current
      hash
    end
  end

  def serialize(object)
    Base64.encode64(Marshal.dump(object))
  end

  def deserialize(string)
    Marshal.load(Base64.decode64(string))
  end

end

Corpus.start